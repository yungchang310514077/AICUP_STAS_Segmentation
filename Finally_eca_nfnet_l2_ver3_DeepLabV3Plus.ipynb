{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install monai\n",
    "# !pip3 install -U Setuptools\n",
    "# # !pip install torch\n",
    "# # !pip install torchvision\n",
    "# !pip install optuna\n",
    "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "# !pip install adabelief-pytorch==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ECA-NFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import setuptools\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import create_test_image_2d, list_data_collate, decollate_batch\n",
    "from monai.inferers import sliding_window_inference, SimpleInferer\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChanneld,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotate90d,\n",
    "    RandZoomd,\n",
    "    RandFlipd,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    AsChannelFirstd,\n",
    "    AsChannelLast,\n",
    "    Resized,\n",
    "    RandScaleCropd,\n",
    "    RandRotated,\n",
    "    SaveImage,\n",
    "    RandSpatialCropd,\n",
    "    Resize,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data folder\n",
    "#### 設定資料夾路徑，其中含有 \"Train_Images\" 與 \"Train_Annotations_png\" 兩個子資料夾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Data folder\n",
    "data_path = './SEG_Train_Datasets/'\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "#### 以字典檔的形式，將每一張圖象(image)對應到其遮罩(mask)，並將字典檔放入列表中，再從列表中切分 training data 與 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = 237\n",
    "\n",
    "tempdir = data_path + \"Train_Images\"\n",
    "train_images = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "\n",
    "tempdir = data_path + \"Train_Annotations_png\"\n",
    "train_segs = sorted(glob.glob(os.path.join(tempdir, \"*.png\")))\n",
    "\n",
    "# training data\n",
    "train_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(train_images[val_data:], train_segs[val_data:])]\n",
    "print(f\" {len(train_images[val_data:])} train_images and {len(train_segs[val_data:])} train_segs\")\n",
    "\n",
    "# validation data\n",
    "val_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(train_images[:val_data], train_segs[:val_data])]\n",
    "print(f\" {len(train_images[:val_data])} val_images and {len(train_segs[:val_data])} val_segs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 data \n",
    "train_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trasform for image and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for image and segmentation\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        AddChanneld(keys=[\"seg\"]),        \n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        # RandZoomd(keys=[\"img\", \"seg\"], prob=0.3)\n",
    "        RandFlipd(keys=[\"img\", \"seg\"], prob=0.3),\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=[800, 800]),\n",
    "        RandRotate90d(keys=[\"img\", \"seg\"], prob=0.3, spatial_axes=[0, 1]),\n",
    "        EnsureTyped(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        AddChanneld(keys=[\"seg\"]),        \n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        Resized(keys=[\"img\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader for train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依照所使用的 GPU 記憶體大小，自行設置 batch_size\n",
    "batch_size = 24\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size= batch_size, collate_fn=list_data_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metric and post-processing\n",
    "####  設定 DiceMetric，與 output 使用的 transform，這部分採用老師所給的範例程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicemetric\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "\n",
    "# output transform\n",
    "post_trans = Compose([EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create visualize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image, cmap= 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built model\n",
    "```\n",
    "Encoder 採用 tu-eca_nfnet_l2\n",
    "\n",
    "Decoder 採用 DeepLabV3Plus \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型名稱\n",
    "encode_mod = 'tu-eca_nfnet_l2_DeepLabV3Plus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.4,               # dropout ratio, default is None\n",
    "    activation=None,           # activation function, default is None\n",
    "    classes=1,                 # define number of output labels\n",
    ")\n",
    "\n",
    "\n",
    "encodes = 'tu-eca_nfnet_l2'\n",
    "model_no = smp.DeepLabV3Plus(encodes, aux_params=aux_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model_no).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adabelief_pytorch import AdaBelief\n",
    "optimizer = AdaBelief(model.parameters(), lr=1e-4, eps=1e-16, betas=(0.9, 0.99), weight_decouple = True, rectify = False, weight_decay = 1e-4)\n",
    "loss_function = monai.losses.DiceLoss(sigmoid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "#### 從老師的範例代碼中，對於訓練次數、儲存路徑等做部分微調"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start a typical PyTorch training\n",
    "total_epochs = 250\n",
    "val_interval = 1\n",
    "best_metric = 100   # 存best\n",
    "best_metric_epoch = -1  # 存best\n",
    "epoch_loss_values = list()   # 空list = []\n",
    "metric_values = list()\n",
    "writer = SummaryWriter()  # tensorflow 專有\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{total_epochs}\")\n",
    "\n",
    "    # 開起 train 模式\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    # 計時\n",
    "    time_start = time.time()\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "\n",
    "        step += 1\n",
    "        time_end = time.time()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size   # 多少epoch\n",
    "        print(f\"Step {step}/{epoch_len}\", end='\\r')\n",
    "\n",
    "        inputs, labels = batch_data[\"img\"].to(device), batch_data[\"seg\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)   # forward\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += float(loss.item())\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    local_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())    \n",
    "    print(\"\\n\", f\"{local_time} epoch {epoch + 1} training average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_images = None\n",
    "            val_labels = None\n",
    "            val_outputs = None\n",
    "            steps = 0\n",
    "            loss_val = 0\n",
    "\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "\n",
    "                val_outputs, _ = model(val_images) #forward\n",
    "                val_outputs = Resize([-1, 1716, 942])(val_outputs)\n",
    "\n",
    "                val_loss = monai.losses.DiceLoss(sigmoid=True)(val_outputs, val_labels)\n",
    "                loss_val += float(val_loss)\n",
    "\n",
    "\n",
    "                # val output 經過轉換\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]                \n",
    "\n",
    "                if  steps < 3 :\n",
    "                    print(\"Loss (Validation) : \", val_loss)\n",
    "                    visualize( \n",
    "                        image=val_images[0].cpu().permute(1,2,0), \n",
    "                        ground_truth_mask=val_labels[0].cpu().permute(1,2,0), \n",
    "                        predicted_mask=val_outputs[0].cpu().permute(1,2,0)\n",
    "\n",
    "                    )  \n",
    "                steps += 1\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            print(\"val_loss = \", loss_val / steps)\n",
    "            val_loss_ave = loss_val / steps\n",
    "\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            metric_values.append(val_loss_ave)\n",
    "\n",
    "            if val_loss_ave < best_metric:\n",
    "                best_metric = val_loss_ave\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), f\"{encode_mod}.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                \"current epoch: {} current val mean dice loss: {:.4f} best val mean dice loss: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, val_loss_ave, best_metric, best_metric_epoch\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            torch.save(model.state_dict(), f\"last_epoch_{encode_mod}.pth\")\n",
    "\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check save model\n",
    "#### 再跑一次所儲存的模型，確認Validation分數與儲存之最佳模型一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"{encode_mod}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dice_metric.reset()\n",
    "with torch.no_grad():\n",
    "    val_images = None\n",
    "    val_labels = None\n",
    "    val_outputs = None\n",
    "    # show_val = True\n",
    "    for val_data in val_loader:\n",
    "        val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "        val_outputs, _ = model(val_images) #forward\n",
    "        val_outputs = Resize([-1, 1716, 942])(val_outputs)\n",
    "        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]                \n",
    "        \n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        print(dice_metric.aggregate())\n",
    "    # aggregate the final mean dice result\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    print(\"metric = \", metric)\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"STAS/demo/Public_Image\"\n",
    "test_images = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "\n",
    "print(f\" {len(test_images)} test_images\")\n",
    "\n",
    "test_files = [{\"img\": img} for img in test_images[:]]\n",
    "test_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\"]),      \n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\"]),\n",
    "        Resized(keys=[\"img\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\"])\n",
    "    ]\n",
    ")\n",
    "test_ds = monai.data.Dataset(data=test_files, transform=test_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=1,  collate_fn=list_data_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "alls[0].split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, test_data in enumerate(test_loader):\n",
    "        test_images = test_data[\"img\"].to(device)\n",
    "    \n",
    "        test_outputs, test_out_label = model(test_images) #forward\n",
    "        test_outputs = Resize([-1, 1716, 942])(test_outputs)\n",
    "\n",
    "        saverPD = SaveImage(output_dir=f\"./Predict_{encode_mod}/outputspub\", output_ext=\".png\", output_postfix=f\"{alls[i].split('/')[-1].split('.')[0]}\",scale=255,separate_folder=False)\n",
    "        saverPD(test_outputs[0].cpu())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = sorted(glob.glob(f\"./Predict_{encode_mod}/outputspub/*.png\"))\n",
    "alls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jss in alls:\n",
    "    os.rename(jss, os.path.join(*jss.split(\"/\")[:-1], jss.split(\"/\")[-1].split(\"_\", 1)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"STAS/demo/Private_Image\"\n",
    "private_images = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "\n",
    "print(f\" {len(private_images)} private_images\")\n",
    "\n",
    "private_files = [{\"img\": img} for img in private_images[:]]\n",
    "private_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\"]),      \n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\"]),\n",
    "        Resized(keys=[\"img\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\"])\n",
    "    ]\n",
    ")\n",
    "test_ds = monai.data.Dataset(data= private_files, transform=test_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=1,  collate_fn=list_data_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "alls[0].split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "steppp = 0\n",
    "with torch.no_grad():\n",
    "    for i, test_data in enumerate(test_loader):\n",
    "        test_images = test_data[\"img\"].to(device)\n",
    "\n",
    "        test_outputs, test_out_label = model(test_images) #forward\n",
    "        test_outputs = Resize([-1, 1716, 942])(test_outputs)\n",
    "\n",
    "        saverPD = SaveImage(output_dir=f\"./Predict_{encode_mod}/outputspri\", output_ext=\".png\", output_postfix=f\"{alls[i].split('/')[-1].split('.')[0]}\",scale=255,separate_folder=False)\n",
    "        saverPD(test_outputs[0].cpu())\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = sorted(glob.glob(f\"./Predict_{encode_mod}/outputspri/*.png\"))\n",
    "alls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jss in alls:\n",
    "    os.rename(jss, os.path.join(*jss.split(\"/\")[:-1], jss.split(\"/\")[-1].split(\"_\", 1)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv Predict_{encode_mod}/outputspri/* Predict_{encode_mod}/outputspub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r Predict_tu-eca_nfnet_l2_DeepLabV3Plus.zip Predict_tu-eca_nfnet_l2_DeepLabV3Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
